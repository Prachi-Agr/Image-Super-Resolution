{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Image super-resolution using GAN \n\nGenerative Adversarial Networks is a Deep Neural Networks architecture based on a game-theoretic approach, where two components of the model, namely a generator and discriminator, try to compete with each other. \n\nHere, generator is trained to generate high resolution images from low resolution images. Discriminator is trained to identify original high resolution image and generated high resolution images. This helps the generator to generate better super-resolution images.\n\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Importing libraries \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras import Input\nfrom keras.applications import VGG19, InceptionResNetV2\nfrom keras.callbacks import TensorBoard\nfrom keras.layers import BatchNormalization, Activation, LeakyReLU, Add, Dense\nfrom keras.layers.convolutional import Conv2D, UpSampling2D\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\nimport glob\nimport time\nimport os\nimport cv2\nimport base64\nimport imageio\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport random\n\nfrom imageio import imread\nfrom skimage.transform import resize as imresize\nfrom copy import deepcopy\nfrom tqdm import tqdm\nfrom pprint import pprint\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Defining hyperparameters\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 100\n\nbatch_size = 8\n\nlow_resolution_shape = (64, 64, 3)\n\nhigh_resolution_shape = (256, 256, 3)\n\ncommon_optimizer = Adam(0.0002, 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/*.*\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generator\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def residual_block(x):\n\n    filters = [64, 64]\n    #filters = [128, 128]\n    kernel_size = 3\n    strides = 1\n    padding = \"same\"\n    momentum = 0.8\n    activation = \"relu\"\n\n    res = Conv2D(filters=filters[0], kernel_size=kernel_size, strides=strides, padding=padding)(x)\n    res = Activation(activation=activation)(res)\n    res = BatchNormalization(momentum=momentum)(res)\n\n    res = Conv2D(filters=filters[1], kernel_size=kernel_size, strides=strides, padding=padding)(res)\n    res = BatchNormalization(momentum=momentum)(res)\n\n    res = Add()([res, x])\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_generator():\n    \n    # Using 16 residual blocks ingenerator\n    residual_blocks = 16\n    momentum = 0.8\n    \n    #Input to generator is Low Resolution image\n    input_shape = (64, 64, 3)\n    \n    # Defining Input layer\n    input_layer = Input(shape=input_shape)\n    \n    # Pre-residual block (Convolution layer-n64s1, ReLu)\n    gen1 = Conv2D(filters=64, kernel_size=9, strides=1, padding='same', activation='relu')(input_layer)\n    \n    # Adding 16 residual blocks\n    res = residual_block(gen1)\n    for i in range(residual_blocks - 1):\n        res = residual_block(res)\n    \n    # Post-residual block: Convolutional layer and batchnorm layer\n    gen2 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(res)\n    gen2 = BatchNormalization(momentum=momentum)(gen2)\n    \n    # Adding pre-residual block(gen1) and the post-residual block(gen2)\n    gen3 = Add()([gen2, gen1])\n    \n    # UpSampling\n    gen4 = UpSampling2D(size=2)(gen3)\n    gen4 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(gen4)\n    gen4 = Activation('relu')(gen4)\n    \n    # UpSampling\n    gen5 = UpSampling2D(size=2)(gen4)\n    gen5 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(gen5)\n    gen5 = Activation('relu')(gen5)\n    \n    # Final convolutional layer after upsampling\n    gen6 = Conv2D(filters=3, kernel_size=9, strides=1, padding='same')(gen5)\n    output = Activation('tanh')(gen6)\n    \n    # Model \n    model = Model(inputs=[input_layer], outputs=[output], name='generator')\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Discriminator\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_discriminator():\n    \n    leakyrelu_alpha = 0.2\n    momentum = 0.8\n    \n    #Input is High Resolution image\n    input_shape = (256, 256, 3)\n    \n    #Defining input layer\n    input_layer = Input(shape=input_shape)\n    \n    #8 Convolutional layers with batch normalization  \n    dis1 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(input_layer)\n    dis1 = LeakyReLU(alpha=leakyrelu_alpha)(dis1)\n\n    dis2 = Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(dis1)\n    dis2 = LeakyReLU(alpha=leakyrelu_alpha)(dis2)\n    dis2 = BatchNormalization(momentum=momentum)(dis2)\n\n    dis3 = Conv2D(filters=128, kernel_size=3, strides=1, padding='same')(dis2)\n    dis3 = LeakyReLU(alpha=leakyrelu_alpha)(dis3)\n    dis3 = BatchNormalization(momentum=momentum)(dis3)\n\n    dis4 = Conv2D(filters=128, kernel_size=3, strides=2, padding='same')(dis3)\n    dis4 = LeakyReLU(alpha=leakyrelu_alpha)(dis4)\n    dis4 = BatchNormalization(momentum=0.8)(dis4)\n\n    dis5 = Conv2D(256, kernel_size=3, strides=1, padding='same')(dis4)\n    dis5 = LeakyReLU(alpha=leakyrelu_alpha)(dis5)\n    dis5 = BatchNormalization(momentum=momentum)(dis5)\n\n    dis6 = Conv2D(filters=256, kernel_size=3, strides=2, padding='same')(dis5)\n    dis6 = LeakyReLU(alpha=leakyrelu_alpha)(dis6)\n    dis6 = BatchNormalization(momentum=momentum)(dis6)\n\n    dis7 = Conv2D(filters=512, kernel_size=3, strides=1, padding='same')(dis6)\n    dis7 = LeakyReLU(alpha=leakyrelu_alpha)(dis7)\n    dis7 = BatchNormalization(momentum=momentum)(dis7)\n\n    dis8 = Conv2D(filters=512, kernel_size=3, strides=2, padding='same')(dis7)\n    dis8 = LeakyReLU(alpha=leakyrelu_alpha)(dis8)\n    dis8 = BatchNormalization(momentum=momentum)(dis8)\n    \n    # Fully connected layer \n    dis9 = Dense(units=1024)(dis8)\n    dis9 = LeakyReLU(alpha=0.2)(dis9)\n    \n    # Final fully connected layer for classification\n    output = Dense(units=1, activation='sigmoid')(dis9)\n    \n    \n    model = Model(inputs=[input_layer], outputs=[output], name='discriminator')\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-trained VGG19 \n\nPre-trained VGG19 will be used for feature extraction from real images and generated images\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_vgg():\n    \n    # Dimension corresponding to High Resolution image\n    input_shape = (256, 256, 3)\n    \n    # Using pre-trained vgg19 trained on 'Imagenet' dataset\n    vgg = VGG19(weights=\"imagenet\")\n    \n    # Taking output from 9th layer\n    vgg.outputs = [vgg.layers[9].output]\n    \n    # Defining input layer\n    input_layer = Input(shape=input_shape)\n    \n    # Extracting features \n    features = vgg(input_layer)\n    \n    # Model\n    model = Model(inputs=[input_layer], outputs=[features])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sampling images\n\nImplementing a function for sampling images"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sample_images(data_dir, batch_size, high_resolution_shape, low_resolution_shape):\n    \n    # Creating list of all images in data_dir\n    all_images = glob.glob(data_dir)\n    \n    # Choosing a random batch of images\n    images_batch = np.random.choice(all_images, size=batch_size)\n\n    low_resolution_images = []\n    high_resolution_images = []\n\n    for img in images_batch:\n        # Getting numpy ndarray of current image\n        img1 = imread(img, as_gray=False, pilmode='RGB')\n        img1 = img1.astype(np.float32)\n        \n        # Resizing image\n        img1_high_resolution = imresize(img1, high_resolution_shape)\n        img1_low_resolution = imresize(img1, low_resolution_shape)\n        \n        # Applying augmentation: random horizontal flip\n        if np.random.random() < 0.5:\n            img1_high_resolution = np.fliplr(img1_high_resolution)\n            img1_low_resolution = np.fliplr(img1_low_resolution)\n\n        high_resolution_images.append(img1_high_resolution)\n        low_resolution_images.append(img1_low_resolution)\n    \n    # Convert lists to numpy ndarrays\n    return np.array(high_resolution_images), np.array(low_resolution_images)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Saving images\n\nImplementing a function to save images"},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_images(low_resolution_image, original_image, generated_image, path):\n\n    #Saving low resolution, high resolution and generated high resolution images in one picture\n    fig = plt.figure()\n    \n    ax = fig.add_subplot(1, 3, 1)\n    ax.imshow(original_image)\n    ax.axis(\"off\")\n    ax.set_title(\"ORIGINAL\")\n    \n    ax = fig.add_subplot(1, 3, 2)\n    ax.imshow(low_resolution_image)\n    ax.axis(\"off\")\n    ax.set_title(\"LOW_RESOLUTION\")\n\n    ax = fig.add_subplot(1, 3, 3)\n    ax.imshow(generated_image)\n    ax.axis(\"off\")\n    ax.set_title(\"GENERATED\")\n\n    plt.savefig(path)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## VGG19 compilation\n\nCompiling the trained vgg19 network"},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg = build_vgg()\nvgg.trainable = False\nvgg.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\nvgg.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Discriminator compilation\n\nCompiling discriminator network"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndiscriminator = build_discriminator()\ndiscriminator.trainable = True\ndiscriminator.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generator build\n\n\nBuilding generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"generator = build_generator()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adversarial model compilation\n\nCompiling a adversarial model that includes generator, discriminator and a pre-trained VGG19 network"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_adversarial_model(generator, discriminator, vgg):\n    \n    # Input layer for high-resolution images\n    input_high_resolution = Input(shape=high_resolution_shape)\n\n    # Input layer for low-resolution images\n    input_low_resolution = Input(shape=low_resolution_shape)\n\n    # Generating high-resolution images from low-resolution images\n    generated_high_resolution_images = generator(input_low_resolution)\n\n    # Extracting feature maps from generated images\n    features = vgg(generated_high_resolution_images)\n    \n    # Making discriminator inside GAN untrainable\n    # In an adversarial network, we don't train the discriminator while the generator is training.\n    discriminator.trainable = False\n    discriminator.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\n\n    # Discriminator will give probability of generated high-resolution image\n    probs = discriminator(generated_high_resolution_images)\n\n    # создадим и скомпилируем сотязательную модель\n    adversarial_model = Model([input_low_resolution, input_high_resolution], [probs, features])\n    adversarial_model.compile(loss=['binary_crossentropy', 'mse'], loss_weights=[1e-3, 1], optimizer=common_optimizer)\n    return adversarial_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adversarial_model = build_adversarial_model(generator, discriminator, vgg)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training loop on CelebA dataset\n\nTraining on CelebA Dataset\n\n"},{"metadata":{},"cell_type":"markdown","source":"We will train SRGAN in 2 stages:\n\n* In the first stage, we train the discriminator.\n* In the second stage we train the adversarial network, inside which we are training the generator, but the discriminator is frozen.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(epochs):\n    d_history = []\n    g_history = []\n    print(\"Epoch:{}\".format(epoch))\n    \n    # Sampling batch of images\n    high_resolution_images, low_resolution_images = sample_images(data_dir=data_dir, batch_size=batch_size,\n                                                                          low_resolution_shape=low_resolution_shape,\n                                                                          high_resolution_shape=high_resolution_shape)\n    \n    # Normalizing images\n    high_resolution_images = high_resolution_images / 127.5 - 1.\n    low_resolution_images = low_resolution_images / 127.5 - 1.\n    \n    # Generating high-resolution images from low-resolution images\n    generated_high_resolution_images = generator.predict(low_resolution_images)\n    \n    # Generating a batch of real and fake tags\n    real_labels = np.ones((batch_size, 16, 16, 1))\n    fake_labels = np.zeros((batch_size, 16, 16, 1))\n    \n    # Train generator on real and fake images\n    d_loss_real = discriminator.train_on_batch(high_resolution_images, real_labels)\n    d_loss_real =  np.mean(d_loss_real)\n    d_loss_fake = discriminator.train_on_batch(generated_high_resolution_images, fake_labels)\n    d_loss_fake =  np.mean(d_loss_fake)\n    # Calculating the total loss of the discriminator as the arithmetic average of losses on real and fake tags\n    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n    d_history.append(d_loss)\n    print(\"D_loss:\", d_loss)\n    \n    \n    # Training the generator\n    \n    # Sampling batch of images\n    high_resolution_images, low_resolution_images = sample_images(data_dir=data_dir, batch_size=batch_size,\n                                                                    low_resolution_shape=low_resolution_shape,\n                                                                    high_resolution_shape=high_resolution_shape)\n    \n    #  Normalizing images\n    high_resolution_images = high_resolution_images / 127.5 - 1.\n    low_resolution_images = low_resolution_images / 127.5 - 1.\n    \n    # Extracting feature maps for true high resolution images\n    image_features = vgg.predict(high_resolution_images)\n    \n    # Training the generator\n    g_loss = adversarial_model.train_on_batch([low_resolution_images, high_resolution_images],\n                                             [real_labels, image_features])\n    g_history.append( 0.5 * (g_loss[1]) )\n    print( \"G_loss:\", 0.5 * (g_loss[1]) )\n    \n    # Save and display image samples\n    if epoch % 20 == 0:\n        high_resolution_images, low_resolution_images = sample_images(data_dir=data_dir, batch_size=batch_size,\n                                                                        low_resolution_shape=low_resolution_shape,\n                                                                        high_resolution_shape=high_resolution_shape)\n        \n        # Normalizing images\n        high_resolution_images = high_resolution_images / 127.5 - 1.\n        low_resolution_images = low_resolution_images / 127.5 - 1.\n\n        generated_images = generator.predict_on_batch(low_resolution_images)\n\n        for index, img in enumerate(generated_images):\n            save_images(low_resolution_images[index], high_resolution_images[index], img,\n                        path=\"/kaggle/working/img_{}_{}\".format(epoch, index))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save models weights\n\nSaving model weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"generator.save_weights(\"/kaggle/working/generator.h5\")\ndiscriminator.save_weights(\"/kaggle/working/discriminator.h5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation mode\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#discriminator = build_discriminator()\n#generator = build_generator()\n\ngenerator.load_weights(\"/kaggle/working/generator.h5\")\ndiscriminator.load_weights(\"/kaggle/working/discriminator.h5\")\n\nhigh_resolution_images, low_resolution_images = sample_images(data_dir=data_dir, batch_size=10,\n                                                                      low_resolution_shape=low_resolution_shape,\n                                                                      high_resolution_shape=high_resolution_shape)\n\nhigh_resolution_images = high_resolution_images / 127.5 - 1.\nlow_resolution_images = low_resolution_images / 127.5 - 1.\n\ngenerated_images = generator.predict_on_batch(low_resolution_images)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"## Save images\n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"for index, img in enumerate(generated_images):\n    save_images(low_resolution_images[index], high_resolution_images[index], img,\n                path=\"/kaggle/working/gen_{}\".format(index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}